{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "Let's train agents to hit a ball back and forth in the Tennis environment from Unity ML-Agents! We'll combine deep neural networks and reinforcement learning using a multi-step variant of deep determinsitic policy gradients to teach agents to collaborate on a goal. We'll build up some theory for our methods and implement them to train and evaluate our agents. Let's dive in!\n",
    "\n",
    "![GIF animation of trained agents in Tennis environment](assets/tennis.gif)\n",
    "\n",
    "**Figure 1**: GIF animation showing two agents trained to hit a tennis ball back and forth using deep reinforcement learning (multi-step deep determinstic policy gradients). \n",
    "\n",
    "Here's a full video showing the trained agent: [https://youtu.be/RLc08qHuOp4](https://youtu.be/RLc08qHuOp4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Environment\n",
    "---\n",
    "\n",
    "The Tennis environment in Unity ML-Agents [1] features a tennis court with a net, a ball, and two rackets. Agents control the rackets, which have two continuous action dimensions between -1 and 1. The first action dimension controls position toward and away from the net. The second action dimension controls vertical movement through a jump action. (This action appears binary, initiating a jump at values greater than 0.5.) The observation space has 8 variables representing position and velocity of the agent and the ball. Each state stacks three consecutive observations to help track movement. \n",
    "\n",
    "Agents receive a positive reward of +0.1 for hitting the ball over the net and a negative reward of -0.01 if the ball hits the ground or out of bounds. Solving the environment requires a max score between the two agents of 0.5 averaged over the subsequent 100 episodes. Episodes end when the ball hits the ground or out of bounds, so the agents are incentivized to rally the ball back and forth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "---\n",
    "\n",
    "Let's first introduce some theory and methods useful for training agents in environments with a high-dimensional state spaces and continuous action spaces. We'll build up from basic policy gradients, to actor-critic methods, to deep deterministic policy gradients (DDPG).\n",
    "\n",
    "### Reinforcement Learning\n",
    "\n",
    "Reinforcement learning (RL) models an agent acting in an environment as a Markov decision process (MDP) with state space $\\mathcal{S}$, action space $\\mathcal{A}$, initial state distribution $p_0(s_0)$, and transition function $p(s',r \\mid s,a)$. The agent observes a current state $s \\in \\mathcal{S}$ and selects an action $a \\in \\mathcal{A}$. Based on environmental dynamics governed by the transition function $p(s',r \\mid s,a)$, which is unknown to the agent, the agent transitions to a subsequent state $s' \\in \\mathcal{S}$ and receives a reward $r$. We aim to use RL to teach the agents to act with policy $\\pi : \\mathcal{S} \\rightarrow \\mathcal{A}$ to maximizing cumulative reward, defined as the return $R = \\sum_{t=0}^\\inf \\gamma^t r_{t+1}$ discounted with rate $\\gamma$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value-based Methods\n",
    "\n",
    "Value-based deep RL methods demonstrated significant progress with the success of deep Q-learning networks (DQN) across a wide range of Atari environments (Minh et al., 2014) [2]. Value-based methods take an indirect approach to learning a policy $\\pi$ by first learning a state or action value function. \n",
    "\n",
    "* State value function $v^\\pi(s) = \\mathbb{E}_\\pi [R \\mid s]$, the expected return $R$ following policy $\\pi$ from state $s$. \n",
    "\n",
    "* Action value function $q^\\pi(s,a) = \\mathbb{E}_\\pi[R \\mid s,a]$, the expected return $R$ for taking action $a$ in state $s$ and following policy $\\pi$. \n",
    "\n",
    "The action value function provides the \"goodness\" of each action in a given state. The policy arises by using a max operation to pick the best action. This approach suits environments with discrete action spaces like Atari, where actions are binary inputs from discrete direction and action buttons. In contrast, a task involving finer control such as hitting a tennis ball involves action signals with a continuous range. Finding the maximum action value over a continuous action space adds an iterative optimization task at every time step, significantly increasing computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradients\n",
    "\n",
    "Policy gradient methods learn to directly mapping observed states to actions that maximize return. Advantages over value function methods include continuous action spaces, learning stochastic policies, and stability of convergence. Disadvantages include potential for getting stuck in a local optima, poor sample efficiency, and slow convergence.\n",
    "\n",
    "A neural network function approximator can represent a policy by taking in state values and outputting actions over a continuous range. The agent can learn a parameterized policy $\\pi_\\theta(a|s)$ for the probability of action $a$ given state $s$ by optimizing parameter $\\theta$, where the objective function $J(\\theta)$ is the expected return. The environmental dynamics are unknown, so the agent samples returns from the environment to estimate the expected value. Furthermore, the objective of expected return is also the previously mentioned state value $v^\\pi(s)$, which can be expressed as the probability-weighted sum of action values.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& J(\\theta) = \\mathbb{E}_\\pi \\left[R_t|s_t=s\\right] \\\\\n",
    "& J(\\theta) = v^\\pi(s) \\\\\n",
    "& J(\\theta) = \\sum_{a \\in \\mathcal{A}} \\pi_\\theta (a|s) q^\\pi(s,a)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Gradient ascent can maximize the objective function $J(\\theta)$ by iteratively updating parameters $\\theta$ at a learning rate $\\alpha$ in the direction of the gradient $\\nabla_\\theta J(\\theta)$:\n",
    "\n",
    "$$ \\theta_{t+1} \\leftarrow \\theta_t + \\alpha \\nabla_\\theta J(\\theta_t) $$\n",
    "\n",
    "The gradient $\\nabla_\\theta J(\\theta_t) = \\nabla_\\theta \\big[\\sum_{a \\in \\mathcal{A}} \\pi_\\theta (a|s) q^\\pi(s,a) \\big]$ initially appears challenging to obtain. The product rule requires taking the gradient on both the policy $\\pi_\\theta(a|s)$ term (straightforward) and action value $q^\\pi(s,a)$ term (challenging). \n",
    "The gradient of the action value function $\\nabla_\\theta q^\\pi(s,a)$ requires knowing how the parameters $\\theta$ affect the state distribution, since $q^\\pi(s,a) = \\sum_{s',r} p(s',r \\mid s,a) [r + \\gamma v^\\pi(s')]$ includes the transition function. Luckily, the policy gradient theorem [3] provides an expression for $\\nabla_{\\theta} J(\\theta)$ that doesn't require the derivative of the state distribution $d^\\pi(s)$.\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} J(\\theta) \\propto \\sum_{s \\in \\mathcal{S}} d^\\pi(s) \\sum_{a \\in \\mathcal{A}} q^\\pi(s,a) \\nabla_\\theta \\pi_\\theta(a|s) \n",
    "$$\n",
    "\n",
    "Since following policy $\\pi$ results in the appropriate state distribution, we can express the summation over state distribution as the expected value under policy $\\pi$ by sampling $s_t \\sim \\pi$. This form suits our stochastic gradient ascent approach for updating parameters $\\theta$ by allowing us to sample the expectation of the gradient as the agent interacts with the environment.\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_\\pi \\left[ \\sum_{a \\in \\mathcal{A}} q^\\pi(s_t,a) \\nabla_\\theta \\pi_\\theta(a|s_t) \\right]\n",
    "$$\n",
    "\n",
    "We can multiply and divide by $\\pi_\\theta(a|s_t)$ to replace the summation over actions with an expectation under policy $\\pi$, sampling $a_t \\sim \\pi$. Furthermore, we can express the action value function by its definition as the expected discounted return $\\mathbb{E}_\\pi \\left[ R_t \\mid s_t, a_t \\right]$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_\\pi \\left[ \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(a|s_t) q^\\pi(s,a) \\frac{\\nabla_\\theta \\pi_\\theta(a|s_t)}{\\pi_\\theta(a|s_t)} \\right] \\\\\n",
    "&\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_\\pi \\left[ q^\\pi(s_t,a_t) \\frac{\\nabla_\\theta \\pi_\\theta(a_t|s_t)}{\\pi_\\theta(a_t|s_t)} \\right] \\\\\n",
    "&\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_\\pi \\left[ R_t \\frac{\\nabla_\\theta \\pi_\\theta(a_t|s_t)}{\\pi_\\theta(a_t|s_t)} \\right] \\\\\n",
    "&\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_\\pi \\left[ R_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right] \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The update for parameters $\\theta$ becomes proportional to the return $R_t$, inversely proportional to the action probability $\\pi_\\theta$ (to counteract frequent actions), and in the direction of the gradient $\\nabla_\\theta \\pi_\\theta$. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\theta_{t+1} \\leftarrow \\theta_t +  \\alpha R_t \\frac{\\nabla_\\theta \\pi_{\\theta_t}(a_t|s_t)}{\\pi_{\\theta_t}(a_t|s_t)} \\\\\n",
    "&\\theta_{t+1} \\leftarrow \\theta_t + \\alpha R_t \\nabla_\\theta \\log \\pi_{\\theta_t}(a_t|s_t)  \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This basic policy gradient method is called REINFORCE. Using the Monte Carlo approach of sampling returns to estimate the gradient $\\nabla_\\theta J(\\theta)$ is unbiased but has high variance, making it slow and not sample efficient. Subtracting a baseline from the return is one solution for reducing variance. If the chosen baseline does not vary with action, the expectation doesn't change and remains unbiased. The state value function makes for a good baseline.\n",
    "\n",
    "We can approximate the true value function $v^\\pi(s)$ with a neural network $V_\\omega(s_t)$ parameterized by $\\omega$, learned using stochastic gradient descent with the objective of minimizing the error $\\frac{1}{N} \\sum_i (R_t - V_\\omega(s))^2$ across a mini-batch of $N$ samples. The value and policy parameters update together at their respective learning rates.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\delta_t = R_t - V_\\omega(s_t) \\\\\n",
    "&\\omega_{t+1} \\leftarrow \\omega_t + \\alpha_\\omega \\delta_t \\nabla_\\omega V_\\omega(s_t) \\\\\n",
    "&\\theta_{t+1} \\leftarrow \\theta_t + \\alpha_\\theta \\delta_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The General Advantage Estimation paper by Schulman et al. in 2016 [4] includes an overview of several possible expressions for the policy gradient in Figure 2. Instead of $\\delta_t$ used here, they use the symbol $\\psi_t$ in the paper.\n",
    "\n",
    "![Policy Gradient Expressions](assets/pg_expressions.png)\n",
    "**Figure 2**:  Several expressions for the policy gradient from Schulman et al. [4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic\n",
    "\n",
    "We can further utilize the value function to improve learning by reducing variance at the cost of introducing bias. Actor-critic methods use the value function to not merely provide a baseline. A \"critic\" provides a biased value estimate that replaces full sampled returns to guide the policy, which is the \"actor\". This approach bootstraps a value estimate using an estimate of the subsequent state value obtained with the same function approximator. Bootstrapping reduces variance to speed up learning, but introduces bias due to reliance on an imperfect critic model. \n",
    "\n",
    "For example, an actor-critic method can replace the sampled return $R_t$ with an estimate from the temporal-difference (TD) target $r_{t+1} + \\gamma V_\\omega(s_{t+1})$ using the critic. The network parameters for both the policy (actor) and the value function (critic) are updated with guidance from the critic.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\delta_t = R_t - V_\\omega(s_t) \\\\\n",
    "&\\delta_t = \\left[ r_{t+1} + \\gamma V_\\omega(s_{t+1}) \\right] - V_\\omega(s_t) \\\\\n",
    "&\\omega_{t+1} \\leftarrow \\omega_t + \\alpha_\\omega \\delta_t \\nabla_\\omega V_\\omega(s_t) \\\\\n",
    "&\\theta_{t+1} \\leftarrow \\theta_t + \\alpha_\\theta \\delta_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This approach doesn't require completing a full trajectory like Monte Carlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Deterministic Policy Gradients (DDPG)\n",
    "\n",
    "The actor-critic framework allows us to combine policy gradients with a value-based method like DQN to extend it to continuous action spaces. The techniques introduced in DQN also help deal with issues of error and instability arising from using function approximators with reinforcement learning and policy gradients. \n",
    "\n",
    "DQN builds on Q-learning (Watkins, 1989), a classic off-policy TD algorithm. Q-learning updates tabular Q-values toward a TD target computed using the action $a = \\text{argmax}_{a} Q(s',a)$ that maximizes the Q-value of the subsequent state.\n",
    "\n",
    "$$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\max_a Q(s',a) - Q(s,a)]\n",
    "$$\n",
    "\n",
    "DQN extended Q-learning using deep neural network function approximators for greater representational capacity to handle high-dimensional state spaces. The DQN paper [2] also introduced experience replay buffers and target networks to stabilize learning. \n",
    "\n",
    "DDPG (Lillicrap et al., 2016) combines DQN with policy gradient methods using the actor-critic framework to learn a deterministic policy $\\mu_\\theta(s)$ that acts to approximate Q-learning with guidance from a DQN-like critic $Q_\\omega(s,a)$.\n",
    "\n",
    "#### Deterministic Policy Gradients (DPG)\n",
    "\n",
    "Policy gradient methods can model deterministic policies $\\mu(s)$ in addition to the stochastic policies $\\pi(s|a)$ discussed earlier. Deterministic policy gradient methods have better relative sample efficiency since they don't integrate over action space while stochastic policy methods integrate over both state and action space. The DPG paper (Silver et al., 2014) showed deterministic policy gradients are the expectation of the action value gradient and introduced a deterministic version of the policy gradient theorem to provide an expression for $\\nabla_\\theta J(\\theta)$ that doesn't require the derivative of the state distribution $\\rho^\\mu$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\nabla_\\theta J(\\mu_\\theta) = \\mathbb{E}_{s \\sim \\rho^\\mu} \\left[ \\nabla_{\\theta} q^\\mu(s,a) \\mid _{a=\\mu_\\theta(s)} \\right] \\\\\n",
    "&\\nabla_\\theta J(\\mu_\\theta) = \\mathbb{E}_{s \\sim \\rho^\\mu} \\left[ \\nabla_\\theta \\mu_\\theta (s) \\nabla_a q^\\mu (s,a) \\mid_{a=\\mu_\\theta(s)} \\right] \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Policy parameters $\\theta$ update in proportion to the action value gradient:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} \\leftarrow \\theta_t + \\alpha_\\theta \\nabla_\\theta \\mu_\\theta(s) \\nabla_a q^\\mu(s,a) \\mid_{a=\\mu_\\theta(s)}\n",
    "$$\n",
    "\n",
    "Deterministic policy gradients work with either on-policy or off-policy approaches. For example, an on-policy SARSA update to the critic takes two determinstic actions per time step, using the second action to estimate the value function in the TD target.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\delta_t = \\left[ r_{t+1} + \\gamma Q_\\omega(s_{t+1},a_{t+1}) \\right] - Q_\\omega(s_t,a_t) \\\\\n",
    "&\\omega_{t+1} \\leftarrow \\omega_t + \\alpha_\\omega \\delta_t \\nabla_\\omega Q_\\omega(s_t,a_t) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "DDPG uses an off-policy approach and computes the TD target using policy $\\mu_\\theta(s)$, which approximates the maximization in Q-learning.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\delta_t = \\left[ r_{t+1} + \\gamma Q_\\omega(s_{t+1},\\mu_\\theta (s_{t+1})) \\right] - Q_\\omega(s_t,a_t) \\\\\n",
    "&\\omega_{t+1} \\leftarrow \\omega_t + \\alpha_\\omega \\delta_t \\nabla_\\omega Q_\\omega(s_t,a_t) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Experience Replay Buffer\n",
    "\n",
    "DDPG uses experience replay introduced in DQN to collect experience tuples $\\{(s_i, a_i, s'_i, r_i)\\}$ in a large memory buffer $\\mathcal{B}$. Experiences are sampled for learning update, eliminating temporal correlation of state observations and improving data efficiency by learning from past experiences. Changes in data distribution are also smoothed out by the random sampling from the replay buffer. A common buffer size across literature seems to be 10^6 experience tuples. \n",
    "\n",
    "#### Target Networks\n",
    "\n",
    "DDPG also draws from DQN's target networks: maintaining a separate copy of the network weights — frozen but periodically updated — for estimating action values in the TD target. The TD target computes an updated value estimate using the latest reward plus a bootstrapped value of the next state. Updating a network based on a target generated from the same evolving network can lead to divergence. Using a frozen or slow changing target network provides a stable fixed target for learning updates by smoothing out short term oscillations.\n",
    "\n",
    "#### Soft Updates\n",
    "\n",
    "Target networks for both actor $\\theta'$ and critic $\\omega'$ can update using a soft approach, where the target network weights change gradually rather than being frozen and replaced periodically by the current networks $\\theta$ and $\\omega$. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\theta' \\leftarrow \\tau \\theta + (1-\\tau)\\theta' \\\\\n",
    "&\\omega' \\leftarrow \\tau \\omega + (1-\\tau)\\omega' \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The DDPG paper [6] suggests a hyperparameter value of $\\tau=0.001$, while others [8] use $\\tau=0.005$.\n",
    "\n",
    "#### Batch Normalization\n",
    "\n",
    "The scale and range of state values can vary significantly due to environmental conditions and differences in units (position, velocity). Normalizing each dimension to have unit mean and variance across a mini-batch can significantly improve generalization and learning. As suggested in the DDPG paper [6], I apply batch normalization to the state input and before all layers in the actor network. The critic network also uses batch normalization on the state inputs and before all layers not concatenated with action inputs, since we don't want to alter the action signals.\n",
    "\n",
    "#### Exploratory Action Space Noise\n",
    "\n",
    "Deterministic policies have a harder time attaining sufficient exploration compared to stochastic polices. Continuous action spaces also make exploration important. Adding noise $\\mathcal{N}$ directly to the policy helps encourage exploration.\n",
    "\n",
    "$$\n",
    "\\mu'(s_t) = \\mu_\\theta(s_t) + \\mathcal{N}(0,\\sigma)\n",
    "$$\n",
    "\n",
    "The DDPG paper [6] suggests using noise generated from an Ornstein-Uhlenbeck (OU) process so the exploration is temporally correlated. Other researchers [8,11]  have found plain Gaussian noise works just as well. After implementating unique OU noise processes for each agent, I've also found Gaussian noise with a standard deviation 0.1 works just as well for this task. The actions are clipped between [-1, 1].\n",
    "\n",
    "![DDPG Algorithm](assets/ddpg_algo.png)\n",
    "**Figure 3:** DDPG (deep deterministic policy gradients) algorithm from Lillicrap et al. [6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twin Delayed DDPG (TD3)\n",
    "\n",
    "The TD3 algorithm by Fujimoto et al. in 2018 [8] improves on DDPG with three techniques for handling function approximation error: clipped double Q-learning, delayed policy update, and target policy smoothing. \n",
    "\n",
    "#### Clipped Double Q-learning\n",
    "\n",
    "Q-learning is positively biased due to the max operation used for action selection. For example, taking the max over noisy variables that individually have zero mean can produce an output with positive mean. Double Q-learning (Van Hasselt et al., 2010) and Double DQN (Van Hasselt et al., 2016) deal with the overestimation by maintaining two separate value networks to decouple action selection and evaluation. \n",
    "\n",
    "Going beyond discrete action spaces, the TD3 paper [8] demonstrates overestimation bias also occurs for continous action spaces in the actor-critic framework. Mitigating bias requires decoupling the action selection (policy) from evalution (value function). We therefore want to use the double Q-learning approach for updating value targets using independent critics.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&y_1 = r + \\gamma Q_{\\omega'_2}(s', \\mu_{\\theta_1}(s') ) \\\\\n",
    "&y_2 = r + \\gamma Q_{\\omega'_1}(s', \\mu_{\\theta_2}(s') ) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "However, positive bias still occur for actor-critics with double Q-learning because the critics are not completely independent due to related learning targets and a shared replay buffer. The TD3 paper [8] introduced Clipped Double Q-learning, which uses the minimum of the two critics. The less biased critic becomes upper-bounded by the more biased critic.\n",
    "\n",
    "$$\n",
    "y_1 = r + \\gamma \\min_{i=1,2} Q_{\\omega'_i}(s', \\mu_{\\theta_1}(s') )\n",
    "$$\n",
    "\n",
    "Clipped double Q-learning mitigates overestimation by favoring underestimation. This trade-off makes sense since underestimation bias aren't prone to spread across learning updates.\n",
    "\n",
    "In the Tennis environment, clipped double Q-learning improved learning stabilty but also slowed it down slightly.\n",
    "\n",
    "#### Delayed Policy Updates \n",
    "\n",
    "The TD3 paper [8] highlights the interplay between value and policy updates. Poor value estimates can produce policy updates with divergent behavior. They suggest reducing the frequency of policy updates relative to value updates to allow time for value estimates to improve. This reduces the variance in value estimates used to update the policy, producing a better policy that feeds back into better value estimates. The concept is similar to freezing target networks to reduce error. I tried a delay $d=2$ of two critic updates for every actor update as suggested by the TD3 paper [8], but found learning slowed down.\n",
    "\n",
    "#### Target Policy Smoothing\n",
    "\n",
    "Deterministic policies can overfit narrow peaks in the value function, increasing variance in the TD target. The TD3 paper [8] address the issue using a regularization technique. Since nearby actions should have similar values, adding some noise to the action in the TD target action evalution should help avoid peaks in the value function. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&y = r + \\gamma \\min_{i=1,2} Q_{\\omega'_i}(s', \\mu_{\\theta'}(s')+\\epsilon ) \\\\\n",
    "&\\epsilon \\sim \\text{clip}(\\mathcal{N}(0,\\sigma), -c, c) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The TD3 paper suggests hyperparamaters of $\\sigma=0.2$ clipped at $c=0.5$. For the Tennis environment, I found target policy smoothing did not improve results significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Step Returns\n",
    "\n",
    "Monte Carlo methods calculate returns using full trajectories and are unbiased. However, they have high variance and require many samples. One-step TD methods compute a target using the latest reward plus the subsequent state value estimated using a value function approximator. TD methods reduce variance, but the imperfect value function approximator adds bias. Multi-step returns affect the bias-variance trade-off, reducing bias by incorporating reward samples from a longer trajectory into the TD target. The TD target for n-step return generally has the form:\n",
    "\n",
    "$$\n",
    "y_t = \\sum_{i=t}^{t+N-1} \\gamma^{i-t} r_{i+1} + \\gamma^N V(s_{t+N}, a_{t+N})\n",
    "$$\n",
    "\n",
    "The TD target for our case with clipped double Q-learning and target policy smoothing becomes:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&y_t = \\sum_{i=t}^{t+N-1} \\gamma^{i-t} r_{i+1} + \\gamma^N \\min_{i=1,2} Q_{\\omega'_i}(s'_{t+N}, \\mu_{\\theta'}(s'_{t+N})+\\epsilon ) \\\\\n",
    "&\\epsilon \\sim \\text{clip}(\\mathcal{N}(0,\\sigma), -c, c) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Using a 5-step return showed improved learning compared to a standard single step. The D4PG paper by Barth-Maron et al. in 2018 [11] introduced several improvements for DDPG, including using multi-step returns. (The authors also used multiple distributed parallel actors, a distributional critic, and prioritized experience replay.) By incorporating rewards across longer trajectories into the TD target, multi-step methods seem to help in sparse reward environments such as the initial learning phase in the Tennis environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "---\n",
    "\n",
    "#### Centralized actor and critic networks with shared replay memory\n",
    "The two agents share the same actor and critic networks since they have the same state and action space. The observation variables are defined from the perspective of the agent and relative to the tennis net, so they are symmetric between the two agents. The agents store and learn from experiences in a shared replay buffer.\n",
    "\n",
    "#### Network architecture\n",
    "Architecture for the policy and value networks use suggestions from the DDPG paper [6]. Alternative architectures were not explored here, opting for better generalization rather than fine-tuning for this particular environment. The policy network passes the state input (24 dimensions) into two hidden layers (400 and 300 units) with rectified linear unit (ReLU) activation. State inputs along with inputs to each hidden layer used batch normalization to normalize across mini-batch. Tanh activation on the output produce continous action signals between [-1,1] for each of the two action dimensions.\n",
    "\n",
    "The value network uses a similar architecture but incorporate action inputs in addition to state inputs to represent $Q_\\omega(s,a)$. The state input (24 dimensions) passes to a 400 unit hidden layer with ReLU activation. Two nodes for the action inputs add to the first hidden layer (400 + 2 units) before passing to a second hidden layer (300 units) with ReLU activation. The value network outputs a single unit with ReLU activation to represent $Q_\\omega(s,a)$. Only the state inputs and inputs to the first hidden layer with 400 units use batch normalization, which does not make sense for layers downstream of combined state and action inputs. \n",
    "\n",
    "#### Batch size\n",
    "The policy and value networks learn off-policy by sampling mini-batches from the replay buffer. The DDPG [6] and TD3 [8] papers use batch sizes of 64 and 100, respectively. I found a larger batch size of 1024 improves learning, with the drawback of increased computation time. The D4PG paper [11] also used larger batch sizes of 256 and 512. \n",
    "\n",
    "#### Network update frequency\n",
    "To improve learning stability, learning updates occur only every 100 time steps. Each time, the networks update 50 times, each with a new mini-batch sampled from the replay buffer. The critic network uses only a single gradient step per mini-batch, but perhaps additional gradient steps would produce a worthwhile improvement to the value estimate. \n",
    "\n",
    "#### Gradient clipping\n",
    "Gradient clipping for the critic helps stabilize learning by prevent gradients from becoming too large.\n",
    "\n",
    "#### Initial random exploration\n",
    "Agents take random actions for the initial 5000 time steps to improve exploration and reduce dependency on initial network weights.\n",
    "\n",
    "#### Adam optimizer weight decay\n",
    "The DDPG paper [6] suggested using Adam optimization with zero weight decay for the actor and 0.01 for the critic. A critic weight decay of 0.0001 worked much better for this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Agents\n",
    "---\n",
    "\n",
    "Let's train our agents for the Tennis environment. Success requires a max score of 0.5 between the two agents averaged over 100 subsequent episodes.\n",
    "\n",
    "The agent is implemented in `agent.py`, with the neural networks in `neuralnets.py` and experience replay buffer in `memory.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincentktam/anaconda/envs/drlnd/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Action size: 2\n",
      "State size (per agent): 24\n",
      "States look like:\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.65278625 -1.5\n",
      "  -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.4669857  -1.5\n",
      "   0.          0.         -6.83172083  6.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "from memory import ReplayBuffer\n",
    "from agent import Agent\n",
    "\n",
    "import time\n",
    "import sys\n",
    "from utils import make_label, save_logs, load_pkl\n",
    "\n",
    "random_seed = 0\n",
    "\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Load environment\n",
    "env = UnityEnvironment(file_name='Tennis.app', seed=random_seed)\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "n_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('Number of agents: {}'.format(n_agents))\n",
    "print('Action size: {}'.format(action_size))\n",
    "print('State size (per agent): {}'.format(state_size))\n",
    "print('States look like:\\n{}'.format(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-24 08:45:27  t:     194  Ep:   10  Avg Score:  0.03\n",
      "2018-12-24 08:45:27  t:     367  Ep:   20  Avg Score:  0.02\n",
      "2018-12-24 08:45:28  t:     536  Ep:   30  Avg Score:  0.02\n",
      "2018-12-24 08:45:29  t:     678  Ep:   40  Avg Score:  0.00\n",
      "2018-12-24 08:45:52  t:     857  Ep:   50  Avg Score:  0.01\n",
      "2018-12-24 08:46:03  t:     999  Ep:   60  Avg Score:  0.00\n",
      "2018-12-24 08:46:26  t:    1157  Ep:   70  Avg Score:  0.00\n",
      "2018-12-24 08:46:48  t:    1361  Ep:   80  Avg Score:  0.02\n",
      "2018-12-24 08:47:10  t:    1504  Ep:   90  Avg Score:  0.00\n",
      "2018-12-24 08:47:22  t:    1660 Ep:  100  Avg Score:  0.01  Avg over 100:  0.01\n",
      "2018-12-24 08:47:44  t:    1810 Ep:  110  Avg Score:  0.00  Avg over 100:  0.01\n",
      "2018-12-24 08:47:55  t:    1995 Ep:  120  Avg Score:  0.03  Avg over 100:  0.01\n",
      "2018-12-24 08:48:27  t:    2203 Ep:  130  Avg Score:  0.03  Avg over 100:  0.01\n",
      "2018-12-24 08:48:39  t:    2386 Ep:  140  Avg Score:  0.03  Avg over 100:  0.01\n",
      "2018-12-24 08:49:01  t:    2543 Ep:  150  Avg Score:  0.01  Avg over 100:  0.01\n",
      "2018-12-24 08:49:22  t:    2751 Ep:  160  Avg Score:  0.02  Avg over 100:  0.01\n",
      "2018-12-24 08:49:45  t:    2929 Ep:  170  Avg Score:  0.02  Avg over 100:  0.02\n",
      "2018-12-24 08:50:07  t:    3162 Ep:  180  Avg Score:  0.03  Avg over 100:  0.02\n",
      "2018-12-24 08:50:29  t:    3322 Ep:  190  Avg Score:  0.01  Avg over 100:  0.02\n",
      "2018-12-24 08:50:40  t:    3478 Ep:  200  Avg Score:  0.01  Avg over 100:  0.02\n",
      "2018-12-24 08:51:13  t:    3721 Ep:  210  Avg Score:  0.03  Avg over 100:  0.02\n",
      "2018-12-24 08:51:24  t:    3891 Ep:  220  Avg Score:  0.01  Avg over 100:  0.02\n",
      "2018-12-24 08:51:47  t:    4063 Ep:  230  Avg Score:  0.01  Avg over 100:  0.02\n",
      "2018-12-24 08:52:11  t:    4254 Ep:  240  Avg Score:  0.03  Avg over 100:  0.02\n",
      "2018-12-24 08:52:38  t:    4414 Ep:  250  Avg Score:  0.01  Avg over 100:  0.02\n",
      "2018-12-24 08:52:52  t:    4571 Ep:  260  Avg Score:  0.01  Avg over 100:  0.02\n",
      "2018-12-24 08:53:23  t:    4734 Ep:  270  Avg Score:  0.01  Avg over 100:  0.02\n",
      "2018-12-24 08:53:39  t:    4876 Ep:  280  Avg Score:  0.00  Avg over 100:  0.01\n",
      "2018-12-24 08:54:11  t:    5034 Ep:  290  Avg Score:  0.01  Avg over 100:  0.01\n",
      "2018-12-24 08:54:45  t:    5251 Ep:  300  Avg Score:  0.04  Avg over 100:  0.02\n",
      "2018-12-24 08:55:39  t:    5553 Ep:  310  Avg Score:  0.06  Avg over 100:  0.02\n",
      "2018-12-24 08:56:33  t:    5852 Ep:  320  Avg Score:  0.07  Avg over 100:  0.02\n",
      "2018-12-24 08:57:11  t:    6028 Ep:  330  Avg Score:  0.02  Avg over 100:  0.03\n",
      "2018-12-24 08:58:07  t:    6337 Ep:  340  Avg Score:  0.06  Avg over 100:  0.03\n",
      "2018-12-24 08:58:45  t:    6537 Ep:  350  Avg Score:  0.02  Avg over 100:  0.03\n",
      "2018-12-24 08:59:23  t:    6732 Ep:  360  Avg Score:  0.02  Avg over 100:  0.03\n",
      "2018-12-24 09:00:01  t:    6978 Ep:  370  Avg Score:  0.04  Avg over 100:  0.03\n",
      "2018-12-24 09:00:38  t:    7183 Ep:  380  Avg Score:  0.03  Avg over 100:  0.04\n",
      "2018-12-24 09:01:35  t:    7437 Ep:  390  Avg Score:  0.04  Avg over 100:  0.04\n",
      "2018-12-24 09:02:13  t:    7635 Ep:  400  Avg Score:  0.03  Avg over 100:  0.04\n",
      "2018-12-24 09:02:50  t:    7830 Ep:  410  Avg Score:  0.03  Avg over 100:  0.04\n",
      "2018-12-24 09:03:26  t:    8005 Ep:  420  Avg Score:  0.02  Avg over 100:  0.03\n",
      "2018-12-24 09:04:03  t:    8287 Ep:  430  Avg Score:  0.05  Avg over 100:  0.03\n",
      "2018-12-24 09:04:57  t:    8558 Ep:  440  Avg Score:  0.06  Avg over 100:  0.03\n",
      "2018-12-24 09:05:51  t:    8807 Ep:  450  Avg Score:  0.04  Avg over 100:  0.04\n",
      "2018-12-24 09:07:05  t:    9279 Ep:  460  Avg Score:  0.11  Avg over 100:  0.04\n",
      "2018-12-24 09:08:18  t:    9661 Ep:  470  Avg Score:  0.10  Avg over 100:  0.05\n",
      "2018-12-24 09:09:34  t:   10024 Ep:  480  Avg Score:  0.11  Avg over 100:  0.06\n",
      "2018-12-24 09:10:32  t:   10363 Ep:  490  Avg Score:  0.10  Avg over 100:  0.06\n",
      "2018-12-24 09:12:33  t:   10912 Ep:  500  Avg Score:  0.14  Avg over 100:  0.07\n",
      "2018-12-24 09:13:52  t:   11319 Ep:  510  Avg Score:  0.11  Avg over 100:  0.08\n",
      "2018-12-24 09:14:53  t:   11615 Ep:  520  Avg Score:  0.08  Avg over 100:  0.09\n",
      "2018-12-24 09:15:55  t:   11986 Ep:  530  Avg Score:  0.10  Avg over 100:  0.09\n",
      "2018-12-24 09:17:18  t:   12300 Ep:  540  Avg Score:  0.09  Avg over 100:  0.10\n",
      "2018-12-24 09:18:43  t:   12708 Ep:  550  Avg Score:  0.10  Avg over 100:  0.10\n",
      "2018-12-24 09:20:29  t:   13251 Ep:  560  Avg Score:  0.13  Avg over 100:  0.10\n",
      "2018-12-24 09:21:33  t:   13590 Ep:  570  Avg Score:  0.10  Avg over 100:  0.10\n",
      "2018-12-24 09:24:05  t:   14207 Ep:  580  Avg Score:  0.17  Avg over 100:  0.11\n",
      "2018-12-24 09:26:22  t:   14856 Ep:  590  Avg Score:  0.18  Avg over 100:  0.12\n",
      "2018-12-24 09:28:44  t:   15487 Ep:  600  Avg Score:  0.18  Avg over 100:  0.12\n",
      "2018-12-24 09:30:57  t:   16071 Ep:  610  Avg Score:  0.15  Avg over 100:  0.13\n",
      "2018-12-24 09:35:15  t:   17273 Ep:  620  Avg Score:  0.32  Avg over 100:  0.15\n",
      "2018-12-24 09:36:46  t:   17655 Ep:  630  Avg Score:  0.10  Avg over 100:  0.15\n",
      "2018-12-24 09:38:12  t:   18030 Ep:  640  Avg Score:  0.11  Avg over 100:  0.15\n",
      "2018-12-24 09:40:01  t:   18540 Ep:  650  Avg Score:  0.14  Avg over 100:  0.16\n",
      "2018-12-24 09:41:29  t:   18941 Ep:  660  Avg Score:  0.11  Avg over 100:  0.15\n",
      "2018-12-24 09:43:22  t:   19415 Ep:  670  Avg Score:  0.14  Avg over 100:  0.16\n",
      "2018-12-24 09:44:35  t:   19704 Ep:  680  Avg Score:  0.09  Avg over 100:  0.15\n",
      "2018-12-24 09:45:23  t:   19995 Ep:  690  Avg Score:  0.09  Avg over 100:  0.14\n",
      "2018-12-24 09:47:21  t:   20416 Ep:  700  Avg Score:  0.12  Avg over 100:  0.13\n",
      "2018-12-24 09:48:35  t:   20795 Ep:  710  Avg Score:  0.11  Avg over 100:  0.13\n",
      "2018-12-24 09:50:14  t:   21184 Ep:  720  Avg Score:  0.11  Avg over 100:  0.11\n",
      "2018-12-24 09:54:33  t:   22243 Ep:  730  Avg Score:  0.28  Avg over 100:  0.13\n",
      "2018-12-24 10:00:01  t:   23687 Ep:  740  Avg Score:  0.37  Avg over 100:  0.15\n",
      "2018-12-24 10:10:35  t:   26430 Ep:  750  Avg Score:  0.70  Avg over 100:  0.21\n",
      "2018-12-24 10:32:13  t:   32073 Ep:  760  Avg Score:  1.47  Avg over 100:  0.35\n",
      "2018-12-24 10:51:42  t:   36748 Ep:  770  Avg Score:  1.25  Avg over 100:  0.46\n",
      "Episode 774\tScores: [2.60000004 2.60000004]\n",
      "Solved at episode: 674\n",
      "\n",
      "Run time: 135.34 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate agent\n",
    "buffer_size = int(1e6) \n",
    "batch_size = 1024 \n",
    "memory = ReplayBuffer(action_size, buffer_size, batch_size, random_seed)\n",
    "agent = Agent(state_size, action_size, random_seed, memory, batch_size, \n",
    "            gamma=0.99, \n",
    "            tau=1e-3, \n",
    "            LR_actor=1e-3, \n",
    "            LR_critic=1e-3, \n",
    "            weight_decay=0.0001, \n",
    "            n_agents=n_agents, \n",
    "            update_every=100, \n",
    "            num_updates=50, \n",
    "            doubleQ=False, \n",
    "            delay_policy=1, \n",
    "            smooth_policy=0,\n",
    "            nstep=5, \n",
    "            grad_steps=1)\n",
    "\n",
    "# Append extra label to filenames\n",
    "label_append = ''\n",
    "\n",
    "# Train\n",
    "\n",
    "def train(n_episodes=3000, max_t=0, print_every=10, random_steps=5000):\n",
    "    \"\"\"Train agent\n",
    "    \n",
    "    # Parameters \n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode (up to 1000)\n",
    "        print_every (int): print score every ___ episodes\n",
    "        random_steps (int): number of initial time steps of random\n",
    "    \"\"\"\n",
    "    scores_deque = deque(maxlen=print_every)  # for status updates\n",
    "    scores = []  \n",
    "    avg_score_100 = []  # avg score over next 100\n",
    "    score_window = deque(maxlen=100)\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        t_ep_start = time.time()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]  # reset env    \n",
    "        #agent.reset()  # Reset OU noise processes\n",
    "        states = env_info.vector_observations  # get state(s)\n",
    "        scores_ep = np.zeros(n_agents)  # initialize score(s)\n",
    "        #for t in range(max_t):\n",
    "        while True:\n",
    "            if agent.t < random_steps:\n",
    "                # Initial random exploration\n",
    "                actions = np.random.uniform(-1,1, \n",
    "                                    size=(agent.n_agents, agent.action_size))\n",
    "            else:\n",
    "                actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]  # take action(s) in env\n",
    "            next_states = env_info.vector_observations \n",
    "            rewards = env_info.rewards                         \n",
    "            dones = env_info.local_done  \n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            \"\"\"\n",
    "            # Combine states, actions, rewards across agents\n",
    "            agent.step(states.flatten(), actions.flatten(), \n",
    "                       sum(rewards), next_states.flatten(), dones)\"\"\"\n",
    "            scores_ep += env_info.rewards  # accumulate rewards for return(s)\n",
    "            states = next_states                               \n",
    "            if np.any(dones):  \n",
    "                break\n",
    "        scores_deque.append(np.max(scores_ep)) \n",
    "        scores.append(scores_ep)  \n",
    "        score_window.append(np.max(scores_ep))\n",
    "\n",
    "        if i_episode >= 100:\n",
    "            avg_score_100.append(np.mean(score_window))\n",
    "        print('\\rEpisode {}\\tScores: {}'\\\n",
    "              .format(i_episode, scores_ep), end=\"\")\n",
    "        torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "        torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "\n",
    "        # Stop if solved\n",
    "        if np.mean(score_window) >= 0.5:\n",
    "            print('\\nSolved at episode: {}'.format(i_episode - 100))\n",
    "            break\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            if i_episode < 100:\n",
    "                print('\\r{0:}  t: {1:7.0f}  Ep: {2:4}  Avg Score: {3:5.2f}'\\\n",
    "                    .format(timestamp, agent.t, i_episode, np.mean(scores_deque)))\n",
    "            else:\n",
    "                print('\\r{0:}  t: {1:7.0f}  Ep: {2:4}  Avg Score: {3:5.2f}  Avg over 100: {4:5.2f}'\\\n",
    "                    .format(timestamp, agent.t, i_episode, np.mean(scores_deque), \n",
    "                            np.mean(score_window)))\n",
    "            \n",
    "    return scores\n",
    "\n",
    "t0 = time.time()\n",
    "scores = train()\n",
    "t1 = time.time()\n",
    "print(\"\\nRun time: {:.2f} minutes.\".format((t1-t0)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs saved for: \n",
      "agent_batch1024_updateEvery100_x50_doubleQFalse_delayPol1_smoothPol0_nstep5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "from utils import make_label, save_logs, load_pkl\n",
    "results_dir='results/'\n",
    "label = make_label(agent, label_append)\n",
    "logs = save_logs(scores, label, results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If didn't train, load from logs\n",
    "if 'scores' not in locals():\n",
    "    label = 'agent_batch1024_updateEvery100_x50_doubleQFalse_delayPol1_smoothPol0_nstep5'\n",
    "    filepath = 'results/' + label + '.pkl'\n",
    "    logs = load_pkl(filepath)\n",
    "    scores = logs[label] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize learning and save images\n",
    "\n",
    "# Take average over next 100 scores\n",
    "scores_max = np.amax(scores, axis=1)\n",
    "avg_next_100 = []\n",
    "for i in range(len(scores_max) - 100):\n",
    "    avg_next_100.append(scores_max[i:i+100].mean())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Max score')\n",
    "ax.grid(True)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.plot(np.arange(1, len(scores)+1), [np.max(s_ep) for s_ep in scores])\n",
    "ax.plot(avg_next_100)\n",
    "plt.savefig(results_dir + label + '-' + 'scores.png' + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue learning curve shows the max score between the two agents for each episode. The orange curve shows the average across 100 subsequent episodes. \n",
    "\n",
    "As a sanity check, let's examine the action values and rewards by sampling from the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Visualize action distribution by sampling from memory\n",
    "experiences = random.sample(memory.buffer, k=10000)\n",
    "\n",
    "states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Distribution of Actions')\n",
    "ax.grid(True)   \n",
    "ax = plt.hist(actions.numpy())\n",
    "plt.savefig(results_dir + label + '-' + 'histogram_actions' + '.png')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Distribution of Rewards')\n",
    "ax.grid(True)   \n",
    "ax = plt.hist(rewards.numpy())\n",
    "plt.savefig(results_dir + label + '-' + 'histogram_rewards' + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action histogram shows a reasonable distribution of actions across the continuous range between [-1,1]. Most actions occur at the min and max limits, but a good portion occur in between. This also suggests the action space noise is scaled sensibly.  \n",
    "\n",
    "The reward histogram shows the majority of transitions result in low rewards (0 or -0.01 points). A smaller fraction come from hitting the ball over the net (+0.1 points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Agents\n",
    "---\n",
    "\n",
    "Now let's use the trained agents to interact each other and the Tennis environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch trained agent \n",
    "\n",
    "# Instantiate agent\n",
    "buffer_size = int(1e6) \n",
    "batch_size = 1024 \n",
    "memory = ReplayBuffer(action_size, buffer_size, batch_size, random_seed)\n",
    "agent = Agent(state_size, action_size, random_seed, memory, batch_size, \n",
    "            gamma=0.99, \n",
    "            tau=1e-3, \n",
    "            LR_actor=1e-3, \n",
    "            LR_critic=1e-3, \n",
    "            weight_decay=0.0001, \n",
    "            n_agents=n_agents, \n",
    "            update_every=100, \n",
    "            num_updates=50, \n",
    "            doubleQ=False, \n",
    "            delay_policy=1, \n",
    "            smooth_policy=0,\n",
    "            nstep=5, \n",
    "            grad_steps=1)\n",
    "\n",
    "# Load policy net weights saved from training\n",
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "\n",
    "env_info = env.reset(train_mode=False,)[brain_name]  # reset the env\n",
    "states = env_info.vector_observations  # get state(s)\n",
    "\n",
    "for t in range(1000):\n",
    "    actions = agent.act(states, add_noise=False)  # no action noise\n",
    "    env_info = env.step(actions)[brain_name]  # take action(s) in env\n",
    "    states = env_info.vector_observations  # get next state(s)\n",
    "    rewards = env_info.rewards  \n",
    "    dones = env_info.local_done                    \n",
    "    if np.any(dones):  \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GIF animation in Figure 1 previews the first several time steps. Here's a full video showing the trained agents across 1000 time steps: [https://youtu.be/RLc08qHuOp4](https://youtu.be/RLc08qHuOp4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion & Next Steps\n",
    "---\n",
    "\n",
    "We successfully built agents trained using a multi-step variant of DDPG to hit a tennis ball back and forth without hitting the ground or out of bounds. Slow initial learning was a challenge. The learning curve shows learning starts slowly for a while but takes-off abruptly. The +0.1 reward for hitting the ball over the net occurs infrequently. Using random exploratory actions at the start helps to collect useful experiences to seed learning. Using a larger batch size also helps, perhaps due to the sparse positive rewards. Learning instabilities also added a fair amount of challenge. Adjusting the network update to be less frequent helped stabilize learning. Using clipped double Q-learning from the TD3 paper also helped stabilize learning, but the other techniques from TD3 seemed to slow learning. \n",
    "\n",
    "The next steps include comparing learning performance across multiple random seeds in an ablation study to more rigorously gauge how the individual techniques attempted truly impact learning. More importantly, the current approach to learning in a multi-agent environment using common actor and critic networks with a shared replay memory can only work well with identical agents in a collaborative environment. The next step is to extend to an approach like MADDPG from Lowe et al. [12] to handle more complicated environments with different agents and true competition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "---\n",
    "\n",
    "[1] https://github.com/Unity-Technologies/ml-agents\n",
    "\n",
    "[2] Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A., Veness, Joel, Bellemare, Marc G., Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K., Ostrovski, Georg, Petersen, Stig, Beattie, Charles, Sadik, Amir, Antonoglou, Ioannis, King, Helen, Kumaran, Dharshan, Wierstra, Daan, Legg, Shane, and Hassabis, Demis. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 02 2015. URL http://dx.doi.org/10.1038/nature14236.\n",
    "\n",
    "[3] Sutton, R. and Barto, A. Reinforcement Learning: An Introduction. MIT Press. 1998.\n",
    "\n",
    "[4] Schulman, John, Moritz, Philipp, Levine, Sergey, Jordan, Michael, and Abbeel, Pieter. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b.\n",
    "\n",
    "[5] Watkins, C.J.C.H. (1989), Learning from Delayed Rewards (Ph.D. thesis), Cambridge University, 1989. URL http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf\n",
    "\n",
    "[6] Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.\n",
    "\n",
    "[7] Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. Deterministic policy gradient algo- rithms. In International Conference on Machine Learning (ICML), 2014.\n",
    "\n",
    "[8] Fujimoto, S., van Hoof, H., and Meger, D. Addressing function approximation error in actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.\n",
    "\n",
    "[9] Hasselt, H. V. Double Q-learning. In Advances in Neural Information Processing Systems (NIPS), pp. 2613–2621, 2010.\n",
    "\n",
    "[10] Van Hasselt, Hado, Guez, Arthur, and Silver, David. Deep Reinforcement Learning with Double Q-learning. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, 2016. URL http://arxiv.org/abs/1509.06461.\n",
    "\n",
    "[11] Barth-Maron, G., Hoffman, M. W., Budden, D., Dabney, W., Horgan, D., TB, D., Muldal, A., Heess, N., and Lillicrap, T. Distributional policy gradients. In Proceedings of the\n",
    "International Conference on Learning Representations (ICLR), 2018.\n",
    "\n",
    "[12] Lowe, R.; Wu, Y.; Tamar, A.; Harb, J.; Abbeel, P.; and Mordatch, I. 2017. Multi-agent actor-critic for mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
